{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "---\ntitle: \"Lab 1: Bias-Var Tradeoff & Poly Regression\"\nauthor: \"Parijat Dube\"\ndate: \"2/4/2023\"\noutput: pdf_document\n---\n\nPlease submit your finished lab on Canvas as a knitted pdf or html document. The lab is due on Feb 18, 2024. \n\n# Section I: Goal\n\nThe learning objective of this lab is to investigate the important bias-variance tradeoff in a linear regression setting under squared loss. This will be accomplished by running a small simulation study. The required tasks are stated in Section V. \n\n# Section II: Bias-Variance Tradeoff\n\nLet $(x_1,y_1),\\ldots,(x_n,y_n)$ be the training data and denote the trained model by $\\hat{f}(x)$. Consider a single test case $(x_0,y_0)$, which was not used to train $\\hat{f}(x)$.  The mean square error at test case $(x_0,y_0)$ can be decomposed into three parts; i) variance of $\\hat{f}(x_0)$, ii) squared bias of $\\hat{f}(x_0)$, and iii) irreducible error variance $\\text{Var}(\\epsilon_0)$. The decomposition is stated below:\n\\begin{align*}\nMSE(x_0)&=MSE\\hat{f}(x_0)\\\\\n&=E[(y_0-\\hat{f}(x_0))^2]\\\\\n&=\\text{Var}(\\hat{f}(x_0))+(E\\hat{f}(x_0)-f(x_0))^2+\\text{Var}(\\epsilon_0)\n\\end{align*}\n\n# Section III: True Model and Simulated Data\n\nAssume $X$ is not random taking on values in the interval $[4,20]$ and the true relationship between fixed $X$ and response $Y$ is: \n\\[\ny=f(x)+\\epsilon =(x-5)(x-12)+\\epsilon,\n\\]\nwhere $\\epsilon$ is normally distributed with $E(\\epsilon)=0$ and $\\text{Var}(\\epsilon_0)=20^2$. Note that for test case $(x_0,y_0)$, the statistical relation between $x_0$ and $y_0$ is\n\\[\ny_0=f(x_0)+\\epsilon.\n\\]\n\n\nThe function **true.f()** defined in the code chunk below defines $f(x)$ and is evaluated at $x=16$.  \n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "true.f <- function(x) {\n  f.out <- (x-5)*(x-12)\n  return(f.out)\n}\ntrue.f(16)"}, {"cell_type": "markdown", "metadata": {}, "source": "\nThe function **sim.training()** simulates a training dataset of size $n=20$ based on our regression model. The training feature $x$ is hard-coded and takes on equally spaced values over the interval $[4,20]$. The only input of **sim.training()** is the feature test case $x_0$.  The function returns the simulated dataset and the response for test case(s) $x_0$.   \n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "sim.training <- function(x.test=c(16)\n                         ) {\n  # Hard-coded sample size and standard deviation \n  n <- 20\n  sd.error <- 20\n  \n  # Training x vector\n  x <- seq(4,20,length=n)\n  \n  # Simulate training Y based on f(x) and normal error\n  y <- true.f(x)+rnorm(n,sd=sd.error)\n  \n  # Simulate test case \n  y.test <- true.f(x.test)+rnorm(length(x.test),sd=sd.error)\n  \n  # Return a list of two entries: \n    # 1) the dataframe data.train\n    # 2) test respone vector y_0\n  return(list(data.train=data.frame(x=x,y=y),\n              y.test=y.test))\n}"}, {"cell_type": "markdown", "metadata": {}, "source": "\nTo illustrate **sim.training()**, we simulate a  training dataset using **set.seed(1)**.  Two test cases are chosen at $x_0=10,16$. The following code chunk also plots the simulated data with the true relationship $f(x)$ and chosen test cases.  Make sure to run the entire code chunk at once.  \n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Simulate data\nset.seed(1)\nx.test <- c(10,16)\nsim.data.train <- sim.training(x.test=x.test)\nhead(sim.data.train$data.train,4)\nsim.data.train$y.test\n\n# Plot simulated data\nx.plot <- seq(4,20,by=.01)\nx <- sim.data.train$data.train$x\ny <- sim.data.train$data.train$y\nplot(x,y,main=\"Simulated Data\")\n\n# Plot f(x)\nlines(x.plot,true.f(x.plot),lwd=2,col=\"red\")\n\n# Plot test cases\ny.test <- sim.data.train$y.test\nabline(v=x.test,col=\"grey\")\npoints(x.test,y.test,pch=20,cex=1.5,col=\"blue\")\n\n# Legend\nlegend(\"topleft\",legend=c(\"f(x)\",\"Test cases\"),fill=c(\"red\",\"blue\"),cex=.5)"}, {"cell_type": "markdown", "metadata": {}, "source": "\n\n# Section IV: Polynomial Regression and Tuning Parameter\n\nRecall form lecture, the number of features ($p$) in a multiple linear regression model is a tuning parameter.  This naturally extends to polynomial regression as stated in the below model:\n\\[\ny=\\beta_0+\\beta_1x+\\beta_2x^2+\\cdots+\\beta_px^p+\\epsilon\n\\]\nIn this case, the tuning parameter is the degree of our polynomial $p$.  High degree polynomials can approximate any continuous differentiable function. However, too high of a degree can lead to overfitting and poor generalization. \n\nTo fit this model, use the the **lm()** function in conjunction with **poly()**.  Technically we will utilize orthogonal polynomials which is not exactly the same as the polynomial regression model above. The function **predict.test.case()** defined in the code chunk below estimates $y_0$ based on the $p^{th}$ degree trained polynomial, i.e., $\\hat{f}_p(x_0)$. The inputs are; i) the degree of the polynomial **degree**, ii) the training data **data**, and iii) a vector test cases **x.test**. \n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "predict.test.case <- function(degree,\n                              data,\n                              x.test) {\n\n  # Train model \n  model <- lm(y~poly(x,degree=degree),data=data)\n  # Predict test cases\n  y.test.hat <- predict(model,newdata=data.frame(x=x.test))\n  # Return y test case\n  return(y.test.hat)\n}"}, {"cell_type": "markdown", "metadata": {}, "source": "\nTo illustrate **predict.test.case()**, consider estimating $y_0$ for inputs $x_0=10,16$, where the polynomial is trained using our simulated data **sim.data.train**. Notice that **predict.test.case()** can also be used for plotting. \n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "x.plot <- seq(4,20,by=.01)\nx.test <- c(10,16)\n# Predict for degree=1\ny.pred.1 <- predict.test.case(degree=1,\n                              data=sim.data.train$data.train,\n                              x.test=x.test) \ny.plot.1 <- predict.test.case(degree=1,\n                              data=sim.data.train$data.train,\n                              x.test=x.plot) \n# Predict for degree=2\ny.pred.2 <- predict.test.case(degree=2,\n                              data=sim.data.train$data.train,\n                              x.test=x.test) \ny.plot.2 <- predict.test.case(degree=2,\n                              data=sim.data.train$data.train,\n                              x.test=x.plot)\n# Predict for degree=3\ny.pred.3 <- predict.test.case(degree=3,\n                              data=sim.data.train$data.train,\n                              x.test=x.test) \ny.plot.3 <- predict.test.case(degree=3,\n                              data=sim.data.train$data.train,\n                              x.test=x.plot) \n\n# Plot simulated data\nx <- sim.data.train$data.train$x\ny <- sim.data.train$data.train$y\nplot(x,y,main=\"Simulated Data\")\nabline(h=0,lty=2)\n\n# Plot f(x)\nlines(x.plot,true.f(x.plot),lwd=1.5,col=\"red\")\n\n# Plot the estimated curves\nlines(x.plot,y.plot.1,lwd=1.5,col=\"green\")\nlines(x.plot,y.plot.2,lwd=1.5,col=\"purple\")\nlines(x.plot,y.plot.3,lwd=1.5,col=\"orange\")\n\n# Plot test cases\ny.test <- sim.data.train$y.test\nabline(v=x.test,col=\"grey\")\npoints(x.test,y.test,pch=20,cex=1.5,col=\"blue\")\n\n# Plot estimated test cases\npoints(x.test,y.pred.1,pch=20,cex=1.5,col=\"green\")\npoints(x.test,y.pred.2,pch=20,cex=1.5,col=\"purple\")\npoints(x.test,y.pred.3,pch=20,cex=1.5,col=\"orange\")\n\n# Legend\nlegend(\"topleft\",\n       legend=c(\"f(x)\",\"Test Case\",\"Degree 1\",\"Degree 2\",\"Degree 3\"),\n       fill=c(\"red\",\"blue\",\"green\",\"purple\",\"orange\"),\n       cex=.5)"}, {"cell_type": "markdown", "metadata": {}, "source": "\nThe above code is clunky and can easily be refined. To clean up this process, the function **poly.predict()** defined below trains several polynomial models based on a vector of degrees and outputs the predicted response simultaneously. This function vectorizes **predict.test.case()**. The inputs are; i) a vector of degrees **degree.vec**, ii) a vector of $x$ test points **x.test**, iii) a training dataset **data**. The output of **poly.predict()** is a matrix where the row corresponds to the respective test cases. To see this function in action, the below code also evaluates $\\hat{f}_p(x_0)$ at inputs $x_0=10,16$ using polynomial degrees 1,2,3,4. \n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "poly.predict <- function(degree.vec,\n                          data,\n                          x.test) {\n  \n  # Vectorize predict.test.case()\n  pred <- sapply(degree.vec,\n                 predict.test.case,\n                 data=data,\n                 x.test=x.test)\n  \n  # Name rows and columns\n  rownames(pred)  <- paste(\"TestCase\",1:length(x.test),sep=\"\")\n  colnames(pred)  <- paste(\"D\",degree.vec,sep=\"\")\n  # Return\n  return(pred)\n}\n\n# Test function poly.predict()\nx.test <- c(10,16)  \npoly.predict(degree.vec=1:4,\n             data=sim.data.train$data.train,\n             x.test=x.test)"}, {"cell_type": "markdown", "metadata": {}, "source": "\n# Section V: Student Tasks: (1) - (3)\n\nStudents will solve three major tasks in this lab.  The first task is described below.\n\n## Task 1\n\nSimulate **R=1000** training datasets and for each iteration (or each simulated dataset), store the predicted test cases corresponding to inputs $x_0=10,16$.  For each simulated dataset, you must predict $y_0$ using polynomial regression having degrees $p=1,2,3,4,5$. You can easily solve this problem using a loop and calling on the two functions **sim.training()** and **poly.predict()**.\n\nYour final result will be three matrices. The first matrix **mat.pred.1** is the collection of predicted test cases for each degree corresponding to input $x_0=10$.  Similarly, the matrix **mat.pred.2** corresponds to $x_0=16$. The first two matrices are dimension $(5 \\times 1000)$.  The third matrix **y.test.mat** is the collection of all test cases $y_0$ for each simulated dataset. This matrix is dimension $(2 \\times 1000)$. After completing this problem, display the first three columns of each matrix. \n\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Solution goes here -----------"}, {"cell_type": "markdown", "metadata": {}, "source": "\n\n## Task 2\n\nThe second task is to estimate three different quantities based on the simulation from Task (1). For each polynomial degree ($p=1,2,3,4,5$), use the matrices **mat.pred.1**, **mat.pred.2** and **y.test.mat** to estimate:\n\\begin{enumerate}\n\\item The mean square error $MSE(x_0)$ \n\\item The variance $\\text{Var}(\\hat{f}(x_0))$\n\\item The squared bias $(E\\hat{f}(x_0)-f(x_0))^2$\n\\end{enumerate}\nAfter solving this problem, display the 6 vectors of interest. \n\n**Notes**: \n\n1) When estimating the squared bias, students will use **y.test.mat** to estimate $E\\hat{f}(x_0)$ but will also call the function  **true.f()**  for computing $f(x_0)$. Obviously in practice we never know the true relation $f(x)$. \n\n2) To estimate $MSE(x_0)$, you can slightly modify your loop from Task (1) or write a new program that computes $(y_0-\\hat{f}(x_0))^2$ for all $R=1000$ iterations. Then take the average of the resulting vectors.  \n\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Solution goes here -----------"}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Task 3\n\nThe third task requires students to construct a plots showing $MSE(x_0)$, $\\text{Var}(\\hat{f}(x_0))$ and $\\text{Bias}^2(\\hat{f}(x_0))$ as a function of the polynomial degree. There should be two graphics corresponding to the two test cases $x_0=10$ and $x_0=16$.\n\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Solution goes here -----------"}, {"cell_type": "markdown", "metadata": {}, "source": "\n\n"}], "metadata": {"kernelspec": {"display_name": "R", "language": "r", "name": "ir"}, "language_info": {"codemirror_mode": "r", "file_extension": ".r", "mimetype": "text/x-r-source", "name": "r", "pygments_lexer": "r", "version": "4.0.2"}}, "nbformat": 4, "nbformat_minor": 2}